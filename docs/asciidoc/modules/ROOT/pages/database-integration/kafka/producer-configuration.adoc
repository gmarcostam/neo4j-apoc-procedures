== Configuration

You can set the following configuration values in your `neo4j.conf`, here are the defaults.

.neo4j.conf (with default values)
[source]
----
apoc.kafka.bootstrap.servers=localhost:9092
apoc.kafka.acks=1
apoc.kafka.retries=2
apoc.kafka.batch.size=16384
apoc.kafka.buffer.memory=33554432
apoc.kafka.reindex.batch.size=1000
apoc.kafka.session.timeout.ms=15000
apoc.kafka.connection.timeout.ms=10000
apoc.kafka.replication=1
apoc.kafka.linger.ms=1
apoc.kafka.transactional.id=
apoc.kafka.topic.discovery.polling.interval=300000
apoc.kafka.log.compaction.strategy=delete

apoc.kafka.source.topic.nodes.<TOPIC_NAME>=<PATTERN>
apoc.kafka.source.topic.relationships.<TOPIC_NAME>=<PATTERN>
apoc.kafka.source.topic.relationships.<TOPIC_NAME>.key_strategy=<default/all>
apoc.kafka.source.topic.nodes.<TOPIC_NAME>.from.<DB_NAME>=<PATTERN>
apoc.kafka.source.topic.relationships.<TOPIC_NAME>.from.<DB_NAME>=<PATTERN>
apoc.kafka.source.topic.relationships.<TOPIC_NAME>.from.<DB_NAME>.key_strategy=<default/all>
apoc.kafka.source.enabled=<true/false, default=true>
apoc.kafka.source.enabled.from.<DB_NAME>=<true/false, default=true>
apoc.kafka.procedures.enabled.from.<DB_NAME>=<true/false, default=true>
apoc.kafka.source.schema.polling.interval=<MILLIS, the polling interval for getting the schema information>
----

[NOTE]
====
**To use the Kafka transactions please set `kafka.transactional.id` and `kafka.acks` properly**.
Checkout this {url-confluent-blog}/transactions-apache-kafka/[blog post] for further details about transactions in Apache Kafka
====

See the https://kafka.apache.org/documentation/#brokerconfigs[Apache Kafka documentation] for details on these settings.

In case you Kafka broker is configured with `auto.create.topics.enable` to `false`,
all the messages sent to topics that don't exist are discarded;
this because the `KafkaProducer.send()` method blocks the execution, as explained in https://issues.apache.org/jira/browse/KAFKA-3539[KAFKA-3539].
You can tune the custom property `kafka.topic.discovery.polling.interval` in order to
periodically check for new topics into the Kafka cluster so the plugin will be able
to send events to the defined topics.


With `apoc.kafka.log.compaction.strategy=delete` will be generated a sequence of unique keys with Neo4j Streams Source.
instead with `apoc.kafka.log.compaction.strategy=compact` the keys will be adapted to enable
https://kafka.apache.org/documentation.html#compaction[Log Compaction] on the Kafka side.
Please note that delete strategy does not actually delete records, it has this name  to match the topic config `cleanup.policy=delete/compact`.
Namely, the operations which will involve the same nodes or relationships, will have the same key.

When `apoc.kafka.log.compaction.strategy=compact`, for partitioning we leverage internal Kafka mechanism.

xref:database-integration/kafka/message-structure.adoc[See 'message structure' section to see key examples]

=== Multi Database Support

Neo4j 4.0 Enterprise has https://neo4j.com/docs/operations-manual/4.0/manage-databases/[multi-tenancy support],
in order to support this feature you can set for each database instance a configuration suffix with the following pattern
`from.<DB_NAME>` to the properties in your neo4j.conf file.

Following the list of new properties that allows to support multi-tenancy:

[source]
----
apoc.kafka.source.topic.nodes.<TOPIC_NAME>.from.<DB_NAME>=<PATTERN>
apoc.kafka.source.topic.relationships.<TOPIC_NAME>.from.<DB_NAME>=<PATTERN>
apoc.kafka.source.topic.relationships.<TOPIC_NAME>.from.<DB_NAME>.key_strategy=<PATTERN>
apoc.kafka.source.enabled.from.<DB_NAME>=<true/false, default=true>
----

This means that for each db instance you can specify if:

* use the source connector
* the routing patterns

So if you have a instance name `foo` you can specify a configuration in this way:

[source]
----
apoc.kafka.source.topic.nodes.myTopic.from.foo=<PATTERN>
apoc.kafka.source.topic.relationships.myTopic.from.foo=<PATTERN>
apoc.kafka.source.enabled.from.foo=<true/false, default=true>
----

The old properties:

[source]
----
apoc.kafka.source.enabled=<true/false, default=true>
apoc.kafka.source.topic.nodes.<TOPIC_NAME>=<PATTERN>
apoc.kafka.source.topic.relationships.<TOPIC_NAME>=<PATTERN>
apoc.kafka.procedures.enabled=<true/false, default=true>
----

are still valid and they refer to Neo4j's default db instance.

[NOTE]
====
The default database is controlled by Neo4j's *dbms.default_database* configuration property so we're being clear about
which default database applies for this user.
Database names are case-insensitive and normalized to lowercase, and must follow Neo4j database naming rules.
(Reference: https://neo4j.com/docs/operations-manual/current/manage-databases/configuration/#manage-databases-administration)
====

In particular the following property will be used as default value
for non-default db instances, in case of the specific configuration params is not provided:

[source]
----
apoc.kafka.source.enabled=<true/false, default=true>
----

This means that if you have Neo4j with 3 db instances:

* neo4j (default)
* foo
* bar

and you want to enable the Source plugin on all instances,
you can simply omit any configuration about enabling it, you just need to provide the routing configuration for each instance:

[source]
----
apoc.kafka.source.topic.nodes.testTopic=Test{testId}
apoc.kafka.source.topic.nodes.fooTopic.from.foo=Foo{fooId,fooName}
apoc.kafka.source.topic.relationships.barTopic.from.bar=Bar{barId,barName}
----

Otherwise if you want to enable the Source plugin only on `foo` and `bar` instances,
you can do it in this way:

[source]
----
apoc.kafka.source.enabled=false
apoc.kafka.source.enabled.from.foo=true
apoc.kafka.source.enabled.from.bar=true
apoc.kafka.source.topic.nodes.testTopic=Test{testId}
apoc.kafka.source.topic.nodes.fooTopic.from.foo=Foo{fooId,fooName}
apoc.kafka.source.topic.relationships.barTopic.from.bar=Bar{barId,barName}
----

[NOTE]
====
As you can see, if you want to enable the Source plugin only on one or more specific db instances, you have to previously
disable the Source plugin (`apoc.kafka.source.enabled=false`) and then enable it only on the desired instances (i.e. `apoc.kafka.source.enabled.from.foo=true`).
Furthermore, please note that the `apoc.kafka.source.topic.nodes.testTopic=Test{testId}` will not be considered because the Source plugin on the default database instance `neo4j` has been disabled.
====

So in general if you have:

[source]
----
apoc.kafka.source.enabled=true
apoc.kafka.source.enabled.from.foo=false
----

Then Source module is enabled on all databases EXCEPT `foo` (local overrides global)

[NOTE]
====
For example purposes only, imagine a situation like the following:

You have a Neo4j instance, without Neo4j Streams installed, where a database "testdb" was created and populated.
You decide to install the Neo4j Streams plugin because we want to have also our graph data into Kafka.
So you add the following configuration:

[source]
----
apoc.kafka.bootstrap.servers=localhost:9092
apoc.kafka.source.enabled=true
apoc.kafka.sink.enabled=false
apoc.kafka.procedures.enabled=true
apoc.kafka.source.topic.nodes.topicTest.from.testdb=Test{*}
----

Doing so you expect to reflect all the created/updated nodes with label `Test` into the topic `topicTest`.
What actually happens is:

- all the nodes and relationship inserted before the Kafka setup, are reflected into a topic called "testdb" which is created by default with the database name.
- all the created/updated nodes with label `Test`, after the Kafka setup, are reflected into the configured topic `topicTest`.

The second point happens because, since the database "testdb" is already populated, by enabling the Source module (`apoc.kafka.source.enabled=true`) the plugin will create a default topic with the same name as the database name, and it will reflect all the "testdb" data into it.

If you want to turn off this default behaviour you have to disable the "generic" Source module and enable it just for the
database you are interested of:

[source]
----
apoc.kafka.bootstrap.servers=localhost:9092
apoc.kafka.source.enabled=false
apoc.kafka.sink.enabled=false
apoc.kafka.procedures.enabled=true
apoc.kafka.source.enabled.from.test1=true
apoc.kafka.source.topic.nodes.topicTest.from.testdb=Test{*}
----

====

